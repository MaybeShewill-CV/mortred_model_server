[QWEN2-VL]
llm_model_path="../weights/llm/qwen/Qwen2-VL-7B-Instruct/qwen2-vl-7b-instruct-Q4_K_M.gguf"
mmproj_model_path="../weights/llm/qwen/Qwen2-VL-7B-Instruct/qwen2-vl-7b-instruct-vision-fp16.gguf"
n_gpu_layers=99
main_gpu_device=0
vocab_only=false
vision_model_device="cuda"
vision_model_device_id=0

[SAMPLER]
# 0 = disabled, otherwise samplers should return at least min_keep tokens
min_keep=1
# <= 0 to use vocab size
top_k=100
# 1.0 = disabled
top_p=0.95
# 0.0 = disabled
min_p=0.05
# 0.0 = disabled
xtc_probability=0.0
# > 0.5 disables XTC
xtc_threshold=0.10
# typical_p, 1.0 = disabled
typ_p=1.00
# <= 0.0 to sample greedily, 0.0 to not output probabilities
temp=0.3
# 1.0 = disabled
penalty_repeat= 1.00
# 0.0 = disabled
penalty_freq= 0.00
# 0.0 = disabled
penalty_present= 0.00
# 0.0 = disabled;
dry_multiplier= 0.0
# 0.0 = disabled
dry_base=1.75
# tokens extending repetitions beyond this receive penalty
dry_allowed_length=2
# how many tokens to scan for repetitions (0 = disable penalty, -1 = context size)
dry_penalty_last_n=-1
# consider newlines as a repeatable token
penalize_nl=false
ignore_eos=false
# disable performance metrics
no_perf=false

[CONTEXT]
context_size=8192