[LLAMA3]
model_file_path="../weights/llm/llama/Llama-3.3-70B-Instruct/llama-3.3-70B-Instruct-Q4_K_M.gguf"
n_gpu_layers=80
main_gpu_device=0
vocab_only=false

[SAMPLER]
# 0 = disabled, otherwise samplers should return at least min_keep tokens
min_keep=1
# <= 0 to use vocab size
top_k=100
# 1.0 = disabled
top_p=0.95
# 0.0 = disabled
min_p=0.05
# 0.0 = disabled
xtc_probability=0.0
# > 0.5 disables XTC
xtc_threshold=0.10
# typical_p, 1.0 = disabled
typ_p=1.00
# <= 0.0 to sample greedily, 0.0 to not output probabilities
temp=0.8
# 1.0 = disabled
penalty_repeat= 1.00
# 0.0 = disabled
penalty_freq= 0.00
# 0.0 = disabled
penalty_present= 0.00
# 0.0 = disabled;
dry_multiplier= 0.0
# 0.0 = disabled
dry_base=1.75
# tokens extending repetitions beyond this receive penalty
dry_allowed_length=2
# how many tokens to scan for repetitions (0 = disable penalty, -1 = context size)
dry_penalty_last_n=-1
# consider newlines as a repeatable token
penalize_nl=false
ignore_eos=false
# disable performance metrics
no_perf=false

[CONTEXT]
context_size=51200